titanic <- read.csv('D:/data_analytics/Titanic_Data.csv',
header=T, na.strings=c(""))
titanic_cluster <- subset(titanic, select = c(2,3,6,7,8,10))
View(titanic_cluster)
#calculate mean
avg.age <- mean(titanic_cluster$Age, na.rm = T)
#value of avg.age
print(avg.age)
#replacing na with avg.age
titanic_cluster$Age[is.na(titanic$Age)] = avg.age
View(titanic_cluster)
cluster <- kmeans(titanic_cluster, centers = 2, nstart = 10,
iter.max = 10000L)
cluster$centers
distance <- get_dist(titanic_cluster)
fviz_dist(distance, gradient =
list(low = "cyan", mid = "white",
high = "magenta"))
cluster$tot.withinss
# within the cluster sum of squares
wcss <- function(k) {
kmeans(titanic_cluster, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
wcss_values <- map_dbl(k.values, wcss)
print(wcss_values)
plot(k.values, wcss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
cluster_1 <- kmeans(titanic_cluster, centers = 3, nstart = 10)
str(cluster_1)
print(cluster_1)
# Display each rows value of cluster
cluster_1$cluster
# Centroids of clusters
cluster_1$centers
# total wcss
cluster_1$tot.withinss
## Visulaization of Cluster
## Visulaization of Cluster
fviz_cluster(cluster_1, data = titanic_cluster
## Visulaization of Cluster
fviz_cluster(cluster_1, data = titanic_cluster)
## Visulaization of Cluster
fviz_cluster(cluster_1, data = titanic_cluster)
cluster_1 <- kmeans(titanic_cluster, centers = 2, nstart = 10)
str(cluster_1)
print(cluster_1)
# Display each rows value of cluster
cluster_1$cluster
# Centroids of clusters
cluster_1$centers
# total wcss
cluster_1$tot.withinss
## Visulaization of Cluster
fviz_cluster(cluster_1, data = titanic_cluster)
install.packages(c("DT", "forecast", "fpp2", "ggfortify"))
library(readr)
library(dplyr)
library(DT)
library(knitr)
library(forecast)
library(fpp2)
library(ggfortify)
directory <- "D:/data_analytics/nyse/prices.csv"
fundamentals_dir <- "D:/data_analytics/nyse/fundamentals.csv"
nyse_data <- read_csv(directory)
fundamentals_data <- read_csv(fundamentals_dir)
dim(nyse_data)
colnames(nyse_data)
View(nyse_data)
sum(is.na(nyse_data))
nyse_apple <- nyse_data %>%
mutate(fluctuation = close - open) %>%
filter(symbol == "AAPL") %>%
arrange(date)
names(fundamentals_data)[2] = "symbol"
fundamentals_data_apple <- filter(fundamentals_data,symbol == "AAPL")
dim(nyse_apple)
dim(fundamentals_data_apple)
kable(summary(nyse_apple))
datatable(head(nyse_apple,50))
nyse_apple_open_price <- nyse_apple[,c("open")]
nyse_apple_open_price <- nyse_apple_open_price[1:1050,]
appl.ts <- ts(nyse_apple_open_price, start = c(2010,1, 1), frequency = 365)
autoplot(appl.ts,ts.geom = 'bar', fill = "blue") +
labs(x = "Time in Years", y = "Stock Price in $") +
ggtitle("Apple Stock Price")
ggseasonplot(appl.ts,col=rainbow(3), year.labels=TRUE)+
labs(x = "Time in Months", y = "Stock Price in $") +
scale_x_discrete(1:12)  + ggtitle("Season Plot for Apple Share")
ggAcf(appl.ts, lag.max = 20) +
ggtitle("Auto Correlation Function for Apple stock for 20 days")
forecast::gglagplot(appl.ts, lags = 6) +
labs(x = "Stock Price in $", y = "Stock Price in $") +
ggtitle("Lag Plot for 6 days")
logreturns <- log(diff(appl.ts))
qplot(x = logreturns, fill=..count.., geom="histogram") +
ggtitle("Distribution for Log Returns")
mean(logreturns)
autoplot(logreturns, colour = 'red', linetype = 'dashed' )
AR <-arima(appl.ts, order =c(1,0,0))
autoplot(appl.ts, colour = "blue")  +
labs(x = "Time in Years", y = "Stock Price in $") +
ggtitle("Apple Stock Price - Observed Values")
AR_fitted <- appl.ts - residuals(AR)
autoplot(appl.ts, ts.colour = "blue") +
geom_point(aes(y=AR_fitted), col = "red", size = 1) +
ggtitle("Fitted Values on observed Values")
AR <-arima(appl.ts, order =c(1,0,0))
d.forecast <- forecast(AR, level = c(95), h = 100)
autoplot(d.forecast, ts.color = "blue", predict.linetype = 'dashed') +
ggtitle("Predicted Values and Standard Error")
library(ggplot2)
z <- seq(-4.6, 4.6, 0.3)
q <- seq(0.001,0.999,0.001)
plot(z)
class(z)
## Probability Distribution / Normalization of Data
dStandardNormal <- data.frame(Z=z,
Density=dnorm(z, mean=0, sd=1),
Distribution=pnorm(z, mean=0, sd=1))
ggplot(dStandardNormal,aes(x=Z, y=Density)) +
geom_line(color="blue", size=1.2) +
geom_point()
qStandardNormal <- data.frame(Q=q,
Quantile=qnorm(q, mean=0, sd=1))
head(qStandardNormal)
ggplot(qStandardNormal,aes(x=Q, y=Quantile) +
geom_point()
ggplot(dStandardNormal,aes(x=Z, y=Distribution)) +
geom_line(color="blue", size=1.2) +
geom_point()
geom_line(color="blue", size=1.2) +
ggplot(dStandardNormal,aes(x=Z, y=Distribution)) +
geom_line(color="blue", size=1.2) +
geom_point()
library(readr)
library(dplyr)
library(DT)
library(knitr)
library(forecast)
library(fpp2)
library(ggfortify)
directory <- "D:/data_analytics/nyse/prices.csv"
fundamentals_dir <- "D:/data_analytics/nyse/fundamentals.csv"
nyse_data <- read_csv(directory)
fundamentals_data <- read_csv(fundamentals_dir)
dim(nyse_data)
colnames(nyse_data)
View(nyse_data)
sum(is.na(nyse_data))
nyse_apple <- nyse_data %>%
mutate(fluctuation = close - open) %>%
filter(symbol == "AAPL") %>%
arrange(date)
names(fundamentals_data)[2] = "symbol"
fundamentals_data_apple <- filter(fundamentals_data,symbol == "AAPL")
dim(nyse_apple)
dim(fundamentals_data_apple)
kable(summary(nyse_apple))
datatable(head(nyse_apple,50))
View(nyse_apple)
nyse_apple_open_price <- nyse_apple[,c("open")]
nyse_apple_open_price <- nyse_apple_open_price[1:1050,]
appl.ts <- ts(nyse_apple_open_price, start = c(2010,1, 1), frequency = 365)
autoplot(appl.ts,ts.geom = 'bar', fill = "blue") +
labs(x = "Time in Years", y = "Stock Price in $") +
ggtitle("Apple Stock Price")
ggseasonplot(appl.ts,col=rainbow(3), year.labels=TRUE)+
labs(x = "Time in Months", y = "Stock Price in $") +
scale_x_discrete(1:12)  + ggtitle("Season Plot for Apple Share")
ggAcf(appl.ts, lag.max = 20) +
ggtitle("Auto Correlation Function for Apple stock for 20 days")
forecast::gglagplot(appl.ts, lags = 6) +
labs(x = "Stock Price in $", y = "Stock Price in $") +
ggtitle("Lag Plot for 6 days")
logreturns <- log(diff(appl.ts))
qplot(x = logreturns, fill=..count.., geom="histogram") +
ggtitle("Distribution for Log Returns")
mean(logreturns)
autoplot(logreturns, colour = 'red', linetype = 'dashed' )
AR <-arima(appl.ts, order =c(1,0,0))
autoplot(appl.ts, colour = "blue")  +
labs(x = "Time in Years", y = "Stock Price in $") +
ggtitle("Apple Stock Price - Observed Values")
AR_fitted <- appl.ts - residuals(AR)
autoplot(appl.ts, ts.colour = "blue") +
geom_point(aes(y=AR_fitted), col = "red", size = 1) +
ggtitle("Fitted Values on observed Values")
AR <-arima(appl.ts, order =c(1,0,0))
d.forecast <- forecast(AR, level = c(95), h = 100)
autoplot(d.forecast, ts.color = "blue", predict.linetype = 'dashed') +
ggtitle("Predicted Values and Standard Error")
library(ggplot2)
library(caTools)
library(dplyr)
titanic <- read.csv('D:/data_analytics/Titanic_Data.csv',
header=T, na.strings=c(""))
# Number of columns
ncol(titanic)
nrow(titanic)
View(titanic)
head(titanic)
str(titanic)
# Data Ceaning
# sapply(data, function), is.na(x) returns TRUE for NA
sapply(titanic, function(x) sum(is.na(x)))
# unique value of data
sapply(titanic, function(x) length(unique(x)))
## Missing Value
library(Amelia)
# missmap function takes data and shows missing values present with a color map
missmap(titanic, main = "Missing Values vs. Observed")
#selecting the columns which are required
titanic_df <- subset(titanic, select = c(2,3,5,6,7,8,10,12))
ncol(titanic_df)
missmap(titanic_df, main = "Missing Values vs. Observed")
# number of missing values in new table
sum(is.na(titanic_df$Age))
#calculate mean
avg.age <- mean(titanic_df$Age, na.rm = T)
#value of avg.age
print(avg.age)
#replacing na with avg.age
titanic_df$Age[is.na(titanic$Age)] = avg.age
titanic_df$Age
missmap(titanic_df,main="Missing Values Vs Observed")
View(titanic)
# dropping the missing embarked values
titanic_df <- na.omit(titanic_df)
missmap(titanic_df,main="Missing Values Vs Observed")
# new number of rows
nrow(titanic_df)
# number of 'na'
sum(is.na(titanic_df))
# plot of survival based on sex
gg <- ggplot(titanic_df,aes(x=Survived, fill=Sex))
gg + geom_bar(position="dodge") +
labs(title = "Titanic - Male vs Female in Each Class")
## survival based on class
ggplot(titanic,aes(x=Pclass,fill=Sex))+
geom_bar(position="dodge")+
facet_grid(". ~ Survived") +
labs(title = "Titanic - Survived vs Not Survived in Each PClass")
split_data <- sample.split(titanic_df ,SplitRatio = 0.8)
# Training data 80%,value wiil true
train_data = subset(titanic_df, split_data==T)
# Testing data 20%,value wiil false
test_data = subset(titanic_df, split_data==F)
colnames(train_data)
# caret - collection of alorithms
library(caret)
# Decision Tree Classification
# Model
# fit the model
clf_tree <- train(Survived ~ .,
data=train_data,
method="rpart",
trControl = trainControl(method = "cv"))
summary(clf_tree)
res <- predict(clf_tree ,test_data)
summary(res)
print(res)
#Confusion Matrix
table(res>0.3,test_data$Survived)
library(ROCR)
predicted_val = predict(clf_tree, newdata = test_data)
ROCRPred <- prediction(predicted_val,test_data$Survived)
ROCRPref <- performance(ROCRPred,"tpr","fpr")
plot(ROCRPref, colorize=TRUE, print.cutoffs.at=seq(0.1))
plot(clf_tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(clf_tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
plot(clf_tree$finalModel, uniform=TRUE,
main="Classification Tree")
text(clf_tree$finalModel, use.n.=TRUE, all=TRUE, cex=.8)
library (rpart)
library(rattle)
rpart_clf <- rpart(Survived ~ . , data =train_data, method = 'class')
rpart_clf
fancyRpartPlot(rpart_clf, main = "Titanic TREE")
fancyRpartPlot(rpart_clf, main = "Titanic TREE")
library(readr)
library(dplyr)
library(DT)
library(knitr)
library(forecast)
library(fpp2)
library(ggfortify)
directory <- "D:/data_analytics/nyse/prices.csv"
fundamentals_dir <- "D:/data_analytics/nyse/fundamentals.csv"
nyse_data <- read_csv(directory)
fundamentals_data <- read_csv(fundamentals_dir)
dim(nyse_data)
colnames(nyse_data)
View(nyse_data)
sum(is.na(nyse_data))
nyse_apple <- nyse_data %>%
mutate(fluctuation = close - open) %>%
filter(symbol == "AAPL") %>%
arrange(date)
names(fundamentals_data)[2] = "symbol"
fundamentals_data_apple <- filter(fundamentals_data,symbol == "AAPL")
dim(nyse_apple)
dim(fundamentals_data_apple)
kable(summary(nyse_apple))
datatable(head(nyse_apple,50))
nyse_apple_open_price <- nyse_apple[,c("open")]
nyse_apple_open_price <- nyse_apple_open_price[1:1050,]
appl.ts <- ts(nyse_apple_open_price, start = c(2010,1, 1), frequency = 365)
autoplot(appl.ts,ts.geom = 'bar', fill = "blue") +
labs(x = "Time in Years", y = "Stock Price in $") +
ggtitle("Apple Stock Price")
ggseasonplot(appl.ts,col=rainbow(3), year.labels=TRUE)+
labs(x = "Time in Months", y = "Stock Price in $") +
scale_x_discrete(1:12)  + ggtitle("Season Plot for Apple Share")
ggAcf(appl.ts, lag.max = 20) +
ggtitle("Auto Correlation Function for Apple stock for 20 days")
forecast::gglagplot(appl.ts, lags = 6) +
labs(x = "Stock Price in $", y = "Stock Price in $") +
ggtitle("Lag Plot for 6 days")
logreturns <- log(diff(appl.ts))
qplot(x = logreturns, fill=..count.., geom="histogram") +
ggtitle("Distribution for Log Returns")
mean(logreturns)
autoplot(logreturns, colour = 'red', linetype = 'dashed' )
AR <-arima(appl.ts, order =c(1,0,0))
autoplot(appl.ts, colour = "blue")  +
labs(x = "Time in Years", y = "Stock Price in $") +
ggtitle("Apple Stock Price - Observed Values")
## ARIMA model with AR residuals
AR_fitted <- appl.ts - residuals(AR)
# Forecasting
autoplot(appl.ts, ts.colour = "blue") +
geom_point(aes(y=AR_fitted), col = "red", size = 1) +
ggtitle("Fitted Values on observed Values")
# Reformation of ARIMA
AR <-arima(appl.ts, order =c(1,0,0))
# FORECASTED DATA WITH RESIDUALS
d.forecast <- forecast(AR, level = c(95), h = 100)
autoplot(d.forecast, ts.color = "blue", predict.linetype = 'dashed') +
ggtitle("Predicted Values and Standard Error")
library(datasets)     # US Arrests
library(tidyverse)
library(cluster)      # Clustering of data
library(factoextra)   # Visualization of cluster
data("iris")
View(iris)
sum(is.na(iris))
class(iris)
ncol(iris)
colnames(iris)
iris_df <- subset(iris, select = c(1,2,3,4))
cluster <- kmeans(iris_df, centers = 3, nstart = 10,
iter.max = 10000L)
cluster$centers
distance <- get_dist(iris_df)
fviz_dist(distance, gradient =
list(low = "cyan", mid = "white",
high = "magenta"))
cluster$tot.withinss
# within the cluster sum of squares
wcss <- function(k) {
kmeans(iris_df, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
wcss_values <- map_dbl(k.values, wcss)
print(wcss_values)
plot(k.values, wcss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
cluster_1 <- kmeans(iris_df, centers = 3, nstart = 10)
str(cluster_1)
print(cluster_1)
# Display each rows value of cluster
cluster_1$cluster
# Centroids of clusters
cluster_1$centers
# total wcss
cluster_1$tot.withinss
## Visulaization of Cluster
fviz_cluster(cluster_1, data = iris_df)
library(datasets)     # US Arrests
library(tidyverse)
library(cluster)      # Clustering of data
library(factoextra)   # Visualization of cluster
data("iris")
View(iris)
sum(is.na(iris))
class(iris)
ncol(iris)
colnames(iris)
iris_df <- subset(iris, select = c(1,2,3,4))
iris_df <- scale(iris)
cluster <- kmeans(iris_df, centers = 3, nstart = 10,
iter.max = 10000L)
cluster$centers
distance <- get_dist(iris_df)
fviz_dist(distance, gradient =
list(low = "cyan", mid = "white",
high = "magenta"))
cluster$tot.withinss
# within the cluster sum of squares
wcss <- function(k) {
kmeans(iris_df, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
wcss_values <- map_dbl(k.values, wcss)
print(wcss_values)
plot(k.values, wcss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
cluster_1 <- kmeans(iris_df, centers = 3, nstart = 10)
str(cluster_1)
print(cluster_1)
# Display each rows value of cluster
cluster_1$cluster
# Centroids of clusters
cluster_1$centers
# total wcss
cluster_1$tot.withinss
## Visulaization of Cluster
fviz_cluster(cluster_1, data = iris_df)
library(datasets)
library(tidyverse)
library(cluster)
library(factoextra)       # Visualization of clustering
data("USArrests")
View(USArrests)
sum(is.na(USArrests))
cluster <- kmeans(USArrests, centers = 2, nstart = 25,
iter.max = 10000L)
cluster$centers
distance <- get_dist(USArrests)
fviz_dist(distance, gradient =
list(low = "cyan", mid = "white",
high = "magenta"))
cluster$tot.withinss
# within the cluster sum of squares
wcss <- function(k) {
kmeans(USArrests, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wcss_values <- map_dbl(k.values, wcss)
print(wcss_values)
plot(k.values, wcss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
cluster_1 <- kmeans(USArrests, centers = 4, nstart = 10)
str(cluster_1)
print(cluster_1)
# Display each rows value of cluster
cluster_1$cluster
# Centroids of clusters
cluster_1$centers
# total wcss
cluster_1$tot.withinss
## Visulaization of Cluster
fviz_cluster(cluster_1, data = df)
df <- scale(USArrests)
## Visulaization of Cluster
fviz_cluster(cluster_1, data = df)
library(datasets)
library(tidyverse)
library(cluster)
library(factoextra)       # Visualization of clustering
data("USArrests")
View(USArrests)
sum(is.na(USArrests))
df <- scale(USArrests)
cluster <- kmeans(USArrests, centers = 2, nstart = 25,
iter.max = 10000L)
cluster$centers
distance <- get_dist(USArrests)
fviz_dist(distance, gradient =
list(low = "cyan", mid = "white",
high = "magenta"))
cluster$tot.withinss
# within the cluster sum of squares
wcss <- function(k) {
kmeans(USArrests, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wcss_values <- map_dbl(k.values, wcss)
print(wcss_values)
plot(k.values, wcss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
cluster_1 <- kmeans(USArrests, centers = 4, nstart = 10)
str(cluster_1)
print(cluster_1)
# Display each rows value of cluster
cluster_1$cluster
# Centroids of clusters
cluster_1$centers
# total wcss
cluster_1$tot.withinss
## Visulaization of Cluster
fviz_cluster(cluster_1, data = df)
